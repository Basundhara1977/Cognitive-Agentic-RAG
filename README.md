# ğŸ§  Cognitive Agentic RAG System

A Retrieval-Augmented Generation (RAG) based AI system that integrates document retrieval with large language model reasoning to generate context-aware, grounded responses.

This project demonstrates an agentic workflow where retrieval and reasoning are combined to improve answer quality and factual consistency.

---

## ğŸ“Œ Overview

The system loads a document (PDF), processes it into semantic chunks, generates embeddings, stores them in a vector store, and retrieves relevant context during query time. The retrieved context is then passed to a language model to generate accurate and grounded responses.

The project includes both a notebook implementation and a runnable Python application.

---

## ğŸš€ Features

* Document ingestion from PDF
* Text chunking and embedding generation
* Semantic similarity-based retrieval
* LLM-powered response generation
* Modular RAG pipeline architecture
* Notebook and application-based implementation

---

## ğŸ›  Tech Stack

* Python
* LangChain
* Groq LLM
* Vector Embeddings
* Jupyter Notebook

---

## ğŸ“‚ Project Structure

```
Cognitive-Agentic-RAG/
â”‚
â”œâ”€â”€ agentic_rag.ipynb
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ 2026 Agentic RAG.pdf
â””â”€â”€ .gitignore
```

---

## âš™ï¸ Installation

Clone the repository:

```
git clone https://github.com/Basundhara1977/Cognitive-Agentic-RAG.git
cd Cognitive-Agentic-RAG
```

Install dependencies:

```
pip install -r requirements.txt
```

---

## ğŸ” Environment Setup

Create a `.env` file in the project root directory:

```
GROQ_API_KEY=your_groq_api_key
LANGCHAIN_API_KEY=your_langchain_api_key
```

Do not commit the `.env` file to version control.

---

## â–¶ Running the Project

To run the notebook:

```
jupyter notebook
```

To run the application:

```
python app.py
```

---

## ğŸ§  How It Works

1. Load and preprocess the PDF document
2. Split text into manageable chunks
3. Generate embeddings for each chunk
4. Store embeddings in a vector database
5. Retrieve relevant chunks based on user query
6. Pass retrieved context to the LLM for response generation

This ensures responses are grounded in the provided document rather than relying solely on model memory.

---

## ğŸ“ˆ Future Enhancements

* Multi-document support
* Conversational memory integration
* Web interface improvements
* Cloud deployment

---

## ğŸ‘©â€ğŸ’» Author

Basundhara Maity
